{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "k7l3-mDNzujP"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 27\n",
        "d=2\n",
        "c=3\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "X = np.ones(n*d).reshape(n,d)\n",
        "W = np.ones(d*c).reshape(d,c)\n",
        "B = np.ones(n*c).reshape(n,c) \n",
        "b = np.zeros(c)\n",
        "print(W.shape,B.shape,X.shape)\n",
        "K = sum(sigmoid(np.dot(X,W)+B))\n",
        "print(np.repeat(b,n).reshape(n,c).shape)\n",
        "def softmax(z):\n",
        "  n,d = z.shape\n",
        "  matrix_a = np.zeros(n*d).reshape(n,d)\n",
        "  for i in range(n):\n",
        "    denominator = 0\n",
        "    for j in range(d):\n",
        "      denominator += np.exp(z[i][j])\n",
        "    for j in range(d):\n",
        "      matrix_a[i][j] = np.exp(z[i][j]) / denominator\n",
        "      \n",
        "  return matrix_a\n",
        "\n",
        "# print(softmax(np.array([[1,1,1],[1,1,0],[1,0,0],[2,3,5]])))\n",
        "print((softmax(np.dot(X,W)+B)).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyO8yAfzz0TY",
        "outputId": "9430fab9-917d-4b59-90de-8d4b4f406a37"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 3) (27, 3) (27, 2)\n",
            "(27, 3)\n",
            "27.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "b_HHjWL7zo20"
      },
      "outputs": [],
      "source": [
        "            ###################### Cam NN #####################\n",
        "\n",
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "  return z*(1-z)\n",
        "\n",
        "def softmax(z):\n",
        "  n,d = z.shape\n",
        "  matrix_a = np.zeros(n*d).reshape(n,d)\n",
        "  for i in range(n):\n",
        "    denominator = 0\n",
        "    for j in range(d):\n",
        "      denominator += np.exp(z[i][j])\n",
        "    for j in range(d):\n",
        "      matrix_a[i][j] = np.exp(z[i][j]) / denominator\n",
        "      \n",
        "  return matrix_a\n",
        "\n",
        "\n",
        "class HiddenLayer:\n",
        "  def __init__(self,X,output_nodes=5):\n",
        "    self.alpha = 0.03                    # learning rate\n",
        "    self.X = np.array(X)                 # shape: n*d\n",
        "    self.n,self.d = X.shape\n",
        "    self.c = output_nodes\n",
        "    self.W = np.ones(self.d*self.c).reshape(self.d,self.c)          # shape: d*c\n",
        "    self.b = np.zeros(self.c)            # shape: 1*c\n",
        "    self.B = None\n",
        "    self.A = None\n",
        "    self.dL_dA = np.ones(self.n*self.c).reshape(self.n,self.c)      # shape: n*c\n",
        "    self.dL_dX = None\n",
        "  \n",
        "  def updateX(self,X):\n",
        "    self.X = np.array(X)   # shape: n*d\n",
        "    self.n,self.d = X.shape\n",
        "  \n",
        "  def updateDL_DA(self,dl_da):\n",
        "    self.dL_dA = dl_da\n",
        "  \n",
        "  def returnDL_DA(self):\n",
        "    return self.dL_dA     # shape n*c\n",
        "    \n",
        "  def returnDL_DX(self):\n",
        "    return self.dL_dX     # shape n*c\n",
        "\n",
        "  def FeedForward(self):\n",
        "    self.B = np.repeat(self.b,self.n).reshape(self.n,self.c)          # shape: n*c\n",
        "    self.A = sigmoid(np.dot(self.X,self.W) + self.B)                  # shape: n*c\n",
        "  \n",
        "  def BackPropagte(self):\n",
        "    dA_dZ = sigmoid_derivative(self.A)                                # shape: n*c\n",
        "    db = sum(np.multiply(self.dL_dA,dA_dZ))                           # shape: 1*c\n",
        "    dW = np.dot( self.X.transpose() , np.multiply(self.dL_dA,dA_dZ) ) # shape: d*c\n",
        "    dX = np.dot( np.multiply(self.dL_dA,dA_dZ) , self.W.transpose() ) # shape: n*d    -> dl_da of previous layer\n",
        "    self.W -= self.alpha * dW\n",
        "    self.b -= self.alpha * db\n",
        "    self.dL_dX = dX                                                   # pass to previous layer\n",
        "  \n",
        "  def returnMatrix(self,option):\n",
        "    if option == 'a':\n",
        "      return self.A\n",
        "    elif option == 'w':\n",
        "      return self.W\n",
        "    elif option =='b':\n",
        "      return self.b\n",
        "    elif option =='x':\n",
        "      return self.X\n",
        "    \n",
        "class LastHiddenLayer :\n",
        "  def __init__(self,X,P):\n",
        "    self.alpha = 0.03                    # learning rate\n",
        "    self.X = np.array(X)                 # shape: n*d\n",
        "    self.P = np.array(P) \n",
        "    self.n,self.d = X.shape\n",
        "    self.c = P.shape[1] \n",
        "    self.W = np.ones(self.d*self.c).reshape(self.d,self.c)   # shape: d*c\n",
        "    self.b = np.zeros(self.c)            # shape: 1*c\n",
        "    self.B = None\n",
        "    self.A = None\n",
        "    self.dL_dX = None\n",
        "    self.dL_dZ = None                    # shape: n*c\n",
        "    self.P = np.array(P)                 # shape: n*c\n",
        "\n",
        "  def updateX(self,X):\n",
        "    self.X = np.array(X)                 # shape: n*d\n",
        "    self.n,self.d = X.shape\n",
        "\n",
        "  def returnDL_DX(self):\n",
        "    return self.dL_dX     # shape n*c\n",
        "\n",
        "  def FeedForward(self):\n",
        "    self.B = np.repeat(self.b,self.n).reshape(self.n,self.c)          # shape: n*c\n",
        "    self.A = softmax(np.dot(self.X,self.W) + self.B)                  # shape: n*c\n",
        "  \n",
        "  def BackPropagte(self):                              \n",
        "    self.dL_dZ =  self.A - self.P                   # shape: n*c\n",
        "    db = sum(self.dL_dZ)                            # shape: 1*c\n",
        "    dW = np.dot( self.X.transpose() , self.dL_dZ )  # shape: d*c\n",
        "    dX = np.dot( self.dL_dZ , self.W.transpose() )  # shape: n*d    -> dl_da of previous layer\n",
        "    self.W -= self.alpha * dW\n",
        "    self.b -= self.alpha * db\n",
        "    self.dL_dX = dX                               # pass to previous layer\n",
        "\n",
        "  def LossFunction(self):\n",
        "    return - 1/self.n * (self.P * np.log(self.A)).sum()\n",
        "  \n",
        "  def returnMatrix(self,option):\n",
        "    if option == 'a':\n",
        "      return self.A\n",
        "    elif option == 'w':\n",
        "      return self.W\n",
        "    elif option =='b':\n",
        "      return self.b\n",
        "    elif option =='x':\n",
        "      return self.X\n",
        "\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########  Initialize NN ################\n",
        "\n",
        "X = np.array([[1,2],[3,4],[5,6],[7,8]])        # shape: 4*2 \n",
        "P = np.array([[0,1],[1,0],[1,0],[1,0]])        # shape: 4*2 \n",
        "\n",
        "\n",
        "#################################### 2 epoch only  #################################\n",
        "\n",
        "######### InitNetWork and 1st FORWARD ###########\n",
        "\n",
        "input_layer = HiddenLayer(X,output_nodes=4)        # 4*2  ->  4*4\n",
        "input_layer.FeedForward()\n",
        "# print(input_layer.returnMatrix('a'))\n",
        "\n",
        "first_layer = HiddenLayer(input_layer.returnMatrix('a'),output_nodes=5)        # 4*4  ->  4*5\n",
        "first_layer.FeedForward()\n",
        "# print(first_layer.returnMatrix('a'))\n",
        "\n",
        "second_layer = HiddenLayer(first_layer.returnMatrix('a'),output_nodes=2)       # 4*5  ->  4*2\n",
        "second_layer.FeedForward()\n",
        "# print(second_layer.returnMatrix('a'))\n",
        "\n",
        "last_layer = LastHiddenLayer(second_layer.returnMatrix('a'),P)                  # 4*2  -> 4*2\n",
        "last_layer.FeedForward()\n",
        "print(last_layer.returnMatrix('a'))\n",
        "\n",
        "print('initial loss: ',last_layer.LossFunction())\n",
        "\n",
        "########## 1st BACKWARD ###########\n",
        "last_layer.BackPropagte()\n",
        "second_layer.updateDL_DA(last_layer.returnDL_DX())\n",
        "\n",
        "second_layer.BackPropagte()\n",
        "first_layer.updateDL_DA(second_layer.returnDL_DX())\n",
        "\n",
        "first_layer.BackPropagte()\n",
        "input_layer.updateDL_DA(first_layer.returnDL_DX())\n",
        "\n",
        "input_layer.BackPropagte()\n",
        "\n",
        "##### 2nd FORWARD and Calculate loss 2nd time ##########\n",
        "input_layer.FeedForward()\n",
        "# print(input_layer.returnMatrix('a'))\n",
        "# print(input_layer.returnMatrix('b'))\n",
        "\n",
        "first_layer.updateX(input_layer.returnMatrix('a'))\n",
        "first_layer.FeedForward()\n",
        "# print(first_layer.returnMatrix('a'))\n",
        "# print(first_layer.returnMatrix('b'))\n",
        "\n",
        "second_layer.updateX(first_layer.returnMatrix('a'))\n",
        "second_layer.FeedForward()\n",
        "# print(second_layer.returnMatrix('a'))\n",
        "# print(second_layer.returnMatrix('b'))\n",
        "\n",
        "last_layer.updateX(second_layer.returnMatrix('a'))\n",
        "last_layer.FeedForward()\n",
        "print(last_layer.returnMatrix('a'))\n",
        "print('after 1 epoch loss: ',last_layer.LossFunction())\n",
        "print('We can see that loss actually decrease !')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9f_oOomkD54",
        "outputId": "4a511af8-28e3-452b-ba00-3120439e58ee"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]\n",
            " [0.5 0.5]]\n",
            "initial loss:  0.6931471805599453\n",
            "[[0.52952609 0.47047391]\n",
            " [0.52953002 0.47046998]\n",
            " [0.52953009 0.47046991]\n",
            " [0.52953009 0.47046991]]\n",
            "after 1 epoch loss:  0.6653276949813814\n",
            "We can see that loss actually decrease !\n"
          ]
        }
      ]
    }
  ]
}